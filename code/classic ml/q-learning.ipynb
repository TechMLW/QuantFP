{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf128ec",
   "metadata": {},
   "source": [
    "# DECISION ALGORITHM 1: Q-Learning (Core RL Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841e679",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pennylane pennylane-lightning torch scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c94ceb",
   "metadata": {},
   "source": [
    "## 1A. Classical Q-Learning\n",
    "\n",
    "Topic: Sequential strategic decision-making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbd0c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table: [[-0.14068434  0.42567861]\n",
      " [ 0.16023228 -0.18697587]\n",
      " [ 0.40825992 -0.07792545]\n",
      " [ 0.22774309  0.03506638]\n",
      " [-0.03758372  0.74747005]]\n"
     ]
    }
   ],
   "source": [
    "# TOPIC: Classical Reinforcement Learning - Q-Learning\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "states = 5\n",
    "actions = 2\n",
    "Q = np.zeros((states, actions))\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "\n",
    "for episode in range(200):\n",
    "    state = np.random.randint(states)\n",
    "\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(actions)\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "\n",
    "    reward = np.random.randn()\n",
    "    next_state = np.random.randint(states)\n",
    "\n",
    "    Q[state, action] += alpha * (\n",
    "        reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "    )\n",
    "\n",
    "print(\"Trained Q-table:\", Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ee45f",
   "metadata": {},
   "source": [
    "## 1B. Quantum Q-Learning (Q-Function Approximated by VQC)\n",
    "\n",
    "This is extremely paper-worthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc32be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC: Quantum Reinforcement Learning - Q-Learning with VQC\n",
    "\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n_qubits = 3\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def q_circuit(state, weights):\n",
    "    qml.AngleEmbedding(state, wires=range(n_qubits))\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "weights = torch.nn.Parameter(0.01 * torch.randn(3, n_qubits, 3))\n",
    "optimizer = torch.optim.Adam([weights], lr=0.1)\n",
    "\n",
    "for episode in range(100):\n",
    "    state = torch.rand(n_qubits)\n",
    "    q_value = q_circuit(state, weights)\n",
    "\n",
    "    reward = torch.randn(1)\n",
    "    loss = (q_value - reward) ** 2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593128a",
   "metadata": {},
   "source": [
    "Paper framing:\n",
    "“Instead of a Q-table, the action-value function is approximated by a variational quantum circuit.”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
